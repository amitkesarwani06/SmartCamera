# Frontend-Backend Integration Guide

## âœ… Status: FULLY INTEGRATED

The frontend is **now fully connected** to the backend with real data and functional voice assistant.

---

## ğŸ¯ What's Been Connected

### 1. **Camera Management (LIVE)**
- âœ… Fetches real cameras from backend database on startup
- âœ… Adds new cameras via backend API
- âœ… Deletes cameras from database
- âœ… Updates camera status in real-time

**API Used:** `/cameras` endpoints via `getCameras()`, `createCamera()`, `deleteCamera()`

### 2. **Voice Assistant (FULLY FUNCTIONAL)**
- âœ… Records audio from microphone
- âœ… Sends audio to backend `/voice` endpoint
- âœ… Processes voice commands using:
  - Speech-to-text (Deepgram STT)
  - LLM processing (Qwen2)
  - Command execution logic
- âœ… Returns transcribed text + response
- âœ… Text-to-speech response back to user
- âœ… Handles silence detection (auto-stops after 2 seconds)

**API Used:** `/voice` endpoint via `processVoiceCommand()`

### 3. **API Client Centralization**
- âœ… All API calls now use centralized `client.js`
- âœ… VoiceAssistantFAB updated to use `processVoiceCommand()`
- âœ… Consistent error handling across frontend

---

## ğŸš€ Quick Start

### Prerequisites
- Node.js 16+ (frontend)
- Python 3.10+ (backend)
- Microphone access (browser permission)

### Step 1: Start Backend
```powershell
cd e:\Smartcamera\backend
./setup_backend.bat    # One-time setup only
./start_backend.bat    # Start server
```

Expected output:
```
Starting Backend Server...
Backend will run on: http://localhost:8000
```

### Step 2: Start Frontend
```powershell
cd e:\Smartcamera\frontend
npm install             # One-time setup only
npm run dev            # Start dev server
```

Expected output:
```
VITE v... ready in ... ms
âœ  Local:   http://localhost:5173
```

### Step 3: Open in Browser
Visit: `http://localhost:5173`

---

## ğŸ¤ Testing Voice Assistant

1. **Click the Microphone Button** (bottom-right corner)
2. **Speak a command**, e.g.:
   - "Show me the living room camera"
   - "Add a new place called kitchen"
   - "How many cameras do I have?"
   - "List all places"

3. **Listen for Response** (text-to-speech will play)

### Voice Commands Examples
- `show_camera` - Display a specific camera
- `show_place` - Show cameras in a place
- `add_place` - Create a new place
- `add_camera` - Add a camera
- `analyze_camera` - Analyze camera scene
- `detect_person` - Person detection
- `detect_motion` - Motion detection

---

## ğŸ“Š Data Flow

```
User speaks
    â†“
Microphone captures audio (WebRTC)
    â†“
Frontend records â†’ Sends to /voice endpoint
    â†“
Backend receives audio blob
    â†“
Deepgram STT converts to text
    â†“
LLM (Qwen2) processes command
    â†“
Command executor performs action (query DB/vision)
    â†“
Response sent back to frontend
    â†“
Text-to-speech plays response
```

---

## ğŸ”§ Configuration

### Frontend Environment Variables
Create `.env.local` in `frontend/` folder:
```
VITE_API_BASE_URL=http://localhost:8000
```

### Backend Configuration
- **Port:** 8000
- **Database:** SQLite (database.db)
- **LLM Model:** Qwen2:1b (via Ollama)
- **STT Service:** Deepgram (requires API key in `.env`)

---

## ğŸ› Troubleshooting

### âŒ "Could not reach backend" Error
- Check if backend is running on port 8000
- Verify `VITE_API_BASE_URL` is correct
- Check browser console for CORS issues

### âŒ Microphone Not Working
- Grant permission when browser asks
- Check browser privacy settings
- Ensure HTTPS or localhost (required for microphone access)

### âŒ No Response from Voice Command
- Check backend console for errors
- Verify Deepgram API key is set (if using cloud STT)
- Check if Ollama is running (for LLM processing)

### âŒ Frontend Shows "Failed to load cameras"
- Verify backend is running
- Check network tab in browser DevTools
- Ensure database has cameras (add one via UI)

---

## ğŸ“ Recent Changes

1. **Updated VoiceAssistantFAB.jsx**
   - Removed hardcoded `fetch()` calls
   - Now uses centralized `processVoiceCommand()` from API client
   - Cleaner error handling and consistency

2. **API Client Functions**
   - `processVoiceCommand(audioBlob)` - Send audio and get response
   - `getCameras()` - Fetch all cameras
   - `createCamera(data)` - Add new camera
   - `deleteCamera(id)` - Remove camera
   - `getPlaces()` - Fetch all places
   - `createPlace(data)` - Add new place

---

## âœ¨ Features Enabled

- âœ… Real-time camera list from database
- âœ… Voice-to-command processing
- âœ… Voice response with text-to-speech
- âœ… Silence detection (auto-stop after 2 seconds)
- âœ… Recording indicator UI
- âœ… Error messages with recovery options
- âœ… Microphone permission handling
- âœ… Centralized API client

---

## ğŸ¯ Next Steps (Optional)

1. Add WebSocket support for real-time camera feeds
2. Implement face recognition via vision endpoints
3. Add push notifications for alerts
4. Support additional voice providers (Google, Azure)
5. Improve LLM accuracy with fine-tuning

---

## ğŸ“ Support

Check logs:
- **Frontend:** Browser DevTools Console
- **Backend:** Terminal/Console where you ran `start_backend.bat`
- **Database:** `backend/database.db` (SQLite)

